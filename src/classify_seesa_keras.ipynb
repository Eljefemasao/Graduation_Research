{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graduation Research "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNISTのデータセットを利用して中間層出力検証方法の正確性を検証する(grad_cam検証)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2D_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 300, 300, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 300, 300, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 150, 150, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 150, 150, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 82944)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               42467840  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 43,642,148\n",
      "Trainable params: 43,642,148\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2\n",
      "[[1.2597998e-03 4.3757072e-01 5.6089628e-01 2.7322074e-04]]\n",
      "Tensor(\"max_pooling2d_3/MaxPool:0\", shape=(?, 37, 37, 128), dtype=float32)\n",
      "~~~~~~\n",
      "[<keras.layers.convolutional.Conv2D object at 0x7fe4fe98c550>, <keras.layers.convolutional.Conv2D object at 0x7fe4fe98c2b0>, <keras.layers.convolutional.Conv2D object at 0x7fe4fe9b6f28>, <keras.layers.convolutional.Conv2D object at 0x7fe4fe99b1d0>, <keras.layers.convolutional.Conv2D object at 0x7fe4fe904d30>, <keras.layers.convolutional.Conv2D object at 0x7fe4fe953cf8>, <keras.layers.convolutional.Conv2D object at 0x7fe4fe8d08d0>, <keras.layers.core.Dense object at 0x7fe4fe889e48>, <keras.layers.core.Dense object at 0x7fe4fe8a28d0>]\n",
      "[array([[[[-0.00623139, -0.00772651,  0.00451462],\n",
      "         [-0.00215456, -0.00827836,  0.00861559],\n",
      "         [-0.00185859, -0.00414315,  0.00869981],\n",
      "         ...,\n",
      "         [ 0.00111559,  0.00798083, -0.0068863 ],\n",
      "         [ 0.00510025, -0.00033064, -0.00941131],\n",
      "         [ 0.00075942,  0.00170002, -0.00757856]],\n",
      "\n",
      "        [[-0.00075809, -0.01037271,  0.00561228],\n",
      "         [-0.00097969, -0.00641221,  0.00579803],\n",
      "         [ 0.00279367, -0.01463887,  0.02358115],\n",
      "         ...,\n",
      "         [ 0.00438196,  0.01491827, -0.03257031],\n",
      "         [-0.00863343,  0.00111823, -0.02676674],\n",
      "         [-0.00201142,  0.00864367, -0.00953389]],\n",
      "\n",
      "        [[ 0.00147414, -0.00650845,  0.00760304],\n",
      "         [-0.01416068, -0.01062495,  0.0182291 ],\n",
      "         [ 0.00351393, -0.02315083,  0.03940411],\n",
      "         ...,\n",
      "         [ 0.00503221,  0.01681707, -0.05080236],\n",
      "         [ 0.00413948,  0.00692262, -0.03211997],\n",
      "         [ 0.00916305,  0.00403587, -0.02423632]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.00503962,  0.00257027, -0.00063419],\n",
      "         [-0.00084106,  0.00146628, -0.00754014],\n",
      "         [ 0.00451308, -0.00083467, -0.00650558],\n",
      "         ...,\n",
      "         [ 0.00106882,  0.00532925, -0.01735689],\n",
      "         [-0.00102722,  0.00239328, -0.01369143],\n",
      "         [ 0.00019187,  0.00047071, -0.00654373]],\n",
      "\n",
      "        [[-0.00155979,  0.00345231, -0.00181804],\n",
      "         [ 0.00079648,  0.00123617, -0.00761374],\n",
      "         [ 0.00098308, -0.00128823, -0.00778552],\n",
      "         ...,\n",
      "         [-0.00478403,  0.00723826, -0.01009761],\n",
      "         [-0.00324182,  0.00481778, -0.00643608],\n",
      "         [ 0.00483783,  0.0022936 , -0.00502317]],\n",
      "\n",
      "        [[ 0.00148014,  0.00504107, -0.0018427 ],\n",
      "         [-0.00342576, -0.00117358, -0.00351877],\n",
      "         [ 0.0018243 ,  0.00360456, -0.00445549],\n",
      "         ...,\n",
      "         [-0.00508326,  0.00578993, -0.00733007],\n",
      "         [-0.00145257,  0.00147906, -0.00376837],\n",
      "         [-0.00190967, -0.00066851, -0.0036814 ]]]], dtype=float32)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7fe4fdba2ac8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seimei/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1415, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"/home/seimei/usr/bin/virtualenv/tensorflow-with-gpu/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 526, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import cross_validation\n",
    "import keras.callbacks\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras.preprocessing import image\n",
    "import keras\n",
    "\n",
    "from keras.models import load_model\n",
    "import random as rn\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import model_from_json\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input \n",
    "#https://stackoverflow.com/questions/47555829/preprocess-input-method-in-keras                                                                                                            \n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "#from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from PIL import ImageFile\n",
    "import math\n",
    "#from sklearn.datasets import fetch_mldata\n",
    "\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.framework import ops\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import keras \n",
    "import sys \n",
    "import cv2\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.applications.resnet50 import (ResNet50, preprocess_input)\n",
    "from keras.preprocessing import image\n",
    "% matplotlib inline \n",
    "\n",
    "import sys\n",
    "# clone した convnet-drawer ディレクトリのパスを追加する。\n",
    "sys.path.append('convnet-drawer')\n",
    "from matplotlib_util import save_model_to_file\n",
    "\n",
    "#画像がPILでロードできない問題について\n",
    "# PILは極端に大きな画像など高速にロードできない画像はロードしないで見過ごす仕様になっている故の解決法\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# 使用するGPUの割り当てを決定する\n",
    "config = tf.ConfigProto(device_count={'GPU':0 ,'CPU':56})\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "\n",
    "STANDARD_SIZE = (300, 300)\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)                              \n",
    "\n",
    "input_shape = (300, 300, 3)\n",
    "batch_size = 8\n",
    "epochs = 15\n",
    "num_classes = 4\n",
    "\n",
    "f_log = './logs/object_classificationAB/'\n",
    "f_model = './model/test/'\n",
    "\n",
    "\n",
    "model_filename = 'cnn_model.json'\n",
    "weights_filename = 'cnn_model_weights.hdf5'\n",
    "\n",
    "\n",
    "\n",
    "def input_data(path_train, path_test):\n",
    "\n",
    "    x = []\n",
    "\n",
    "    with open(path_train, \"r\") as f:\n",
    "        train_path_list = f.readlines()\n",
    "\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    for row in train_path_list:\n",
    "        row = row.split(\" \")\n",
    "        filenames.append(row[0])\n",
    "        labels.append(int(row[1]))\n",
    "\n",
    "    for f in filenames:\n",
    "        x.append(image.img_to_array(img_to_matrix(f)))\n",
    "\n",
    "    x = np.asarray(x)\n",
    "    #正規化                                                                                                                 \n",
    "    #x /= 255\n",
    "    x = (x-np.amin(x))/(np.amax(x)-np.amin(x))\n",
    "    y = np.asarray(labels)\n",
    "    y = tf.keras.utils.to_categorical(y, num_classes)\n",
    "\n",
    "    '==============================================================='\n",
    "\n",
    "    a = []\n",
    "    \n",
    "    with open(path_test, \"r\") as f:\n",
    "        train_path_list = f.readlines()\n",
    "\n",
    "    filenames1 = []\n",
    "    labels1 = []\n",
    "    for row in train_path_list:\n",
    "        row = row.split(\" \")\n",
    "        filenames1.append(row[0])\n",
    "        labels1.append(int(row[1]))\n",
    "\n",
    "    for f in filenames1:\n",
    "        a.append(image.img_to_array(img_to_matrix(f)))\n",
    "\n",
    "    test_data = np.asarray(a)\n",
    "    # 正規化                                                                                                                \n",
    "    #test_data /= 255\n",
    "    test_data = (test_data-np.amin(test_data))/(np.amax(test_data)-np.amin(test_data))\n",
    "    test_label = np.asarray(labels1)\n",
    "    test_label = keras.utils.to_categorical(test_label, num_classes)\n",
    "\n",
    "    train_data, valid_data, train_label, valid_label = cross_validation.train_test_split(x, y, test_size=0.3)\n",
    "    test_data, valid1, test_label, valid2 = cross_validation.train_test_split(test_data, test_label, test_size=0.3)\n",
    "    \n",
    "    print(np.shape(train_data))\n",
    "    print(np.shape(train_label))\n",
    "    return train_data, test_data, train_label, test_label, valid_data, valid_label\n",
    "\n",
    "\n",
    "\n",
    "def tf_image_translate(images, tx, ty, interpolation=\"NEAREST\"):\n",
    "    # got these parameters from solving the equations for pixel translations\n",
    "    # on https://www.tensorflow.org/api_docs/python/tf/contrib/image/transform\n",
    "    translations = [1, 0, -tx, 0, 1, -ty, 0, 0]\n",
    "    return tf.contrib.image.translation(images, transforms, interpolation)\n",
    "    \n",
    "    \"\"\"\n",
    "    #augmentations\n",
    "    if is_training():\n",
    "        #color augmentations\n",
    "        image = tf.image.random_brightness(image,max_delta=63)\n",
    "        #image = tf.image.random_saturation(image, lower=0.5, upper=1.5, seed=seed2) \n",
    "        #image = tf.image.random_hue(image, max_delta=0.2, seed=seed3)\n",
    "        image = tf.image.random_contrast(image, lower=0.2, upper=1.8)\n",
    "        \n",
    "        tx = tf.random_normal(shape=[], mean=0.0, stddev=20.0, dtype=tf.float32)#正規分布によるランダム値 \n",
    "        ty = tf.random_normal(shape=[], mean=0.0, stddev=20.0, dtype=tf.float32)\n",
    "        image = tf_image_translate(image, tx=tx, ty=ty)\n",
    "        \n",
    "        degrees = tf.random_normal(shape=[], mean=0.0, stddev=5.0, dtype=tf.float32)\n",
    "        image = tf.contrib.image.rotate(image, degrees*math.pi/180, interpolation=\"BILINEAR\")\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# parse image                                                                                                               \n",
    "def img_to_matrix(filename, verbose=False):\n",
    "    img = Image.open(filename)\n",
    "    if verbose:\n",
    "        print('changing size from %s to %s' % (str(img.size), str(STANDARD_SIZE)))\n",
    "    img = img.resize(STANDARD_SIZE)\n",
    "    imgArray = np.asarray(img)\n",
    "    return imgArray  # imgArray.shape = (167 x 300 x 3)                                                                     \n",
    "\n",
    "\n",
    "    \n",
    "def target_category_loss(x, category_index, nb_classes):\n",
    "    return tf.multiply(x, KTF.one_hot([category_index], nb_classes))\n",
    "\n",
    "def target_category_loss_output_shape(input_shape):\n",
    "    return input_shape\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (KTF.sqrt(KTF.mean(KTF.square(x)) + 1e-5))\n",
    "\n",
    "def load_image(path):\n",
    "    img_path = path\n",
    "    img = image.load_img(img_path, target_size=(300, 300))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    " #   x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "def register_gradient():\n",
    "    if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
    "        @ops.RegisterGradient(\"GuidedBackProp\")\n",
    "        def _GuidedBackProp(op, grad):\n",
    "            dtype = op.inputs[0].dtype\n",
    "            return grad * tf.cast(grad > 0., dtype) *  tf.cast(op.inputs[0] > 0., dtype)\n",
    "\n",
    "def compile_saliency_function(model, activation_layer='conv2d_8'):\n",
    "    \"\"\"\n",
    "    category_index=1\n",
    "    nb_classes = num_classes\n",
    "    target_layer = lambda x: target_category_loss(x, category_index, nb_classes)\n",
    "    s = input_model.layers[-1].output\n",
    "    x1 = Lambda(target_layer, output_shape=target_category_loss_output_shape)(s)\n",
    "    model = keras.models.Model(input_model.layers[0].input, x1)\n",
    "    input_img = model.layers[0].input\n",
    "\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    layer_output =layer_dict[activation_layer].output\n",
    "   #print(layer_output)\n",
    "    max_output = KTF.max(layer_output, axis=3)\n",
    "    saliency = KTF.gradients(KTF.sum(max_output), input_img)[0]\n",
    "    \"\"\"\n",
    "    input_img = model.input\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    layer_output = layer_dict[activation_layer].output\n",
    "    max_output = K.max(layer_output, axis=3)\n",
    "    saliency = K.gradients(K.sum(max_output), input_img)[0]\n",
    "    return K.function([input_img, K.learning_phase()], [saliency])\n",
    "    \n",
    "    #return KTF.function([input_img, KTF.learning_phase()], [saliency])\n",
    "\n",
    "\n",
    "      \n",
    "def modify_backprop(model, name):\n",
    "\n",
    "    g = tf.get_default_graph()\n",
    "\n",
    "    with g.gradient_override_map({'Relu': name}):\n",
    "\n",
    "        # get layers that have an activation\n",
    "        layer_dict = [layer for layer in model.layers[1:]\n",
    "                      if hasattr(layer, 'activation')]\n",
    "        print(layer_dict)\n",
    "        # replace relu activation\n",
    "        for layer1 in layer_dict:\n",
    "            if layer1.activation == keras.activations.relu:\n",
    "                layer1.activation = tf.nn.relu\n",
    "\n",
    "        # re-instanciate a new model\n",
    "        new_model = model\n",
    "    return new_model\n",
    "\n",
    "def deprocess_image(x):\n",
    "    '''\n",
    "    Same normalization as in:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    '''\n",
    "    if np.ndim(x) > 3:\n",
    "        x = np.squeeze(x)\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if KTF.image_dim_ordering() == 'th':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "\n",
    "        \n",
    "def grad_cam(input_model, image, category_index, layer_name):\n",
    "  \n",
    "    nb_classes = num_classes\n",
    "    target_layer = lambda x: target_category_loss(x, category_index, nb_classes)\n",
    "    x = input_model.layers[-1].output\n",
    "    x = Lambda(target_layer, output_shape=target_category_loss_output_shape)(x)\n",
    "    model = keras.models.Model(input_model.layers[0].input, x)\n",
    "\n",
    "    conv_output =model.layers[9].output #model.layers[5].output \n",
    "    print(conv_output)\n",
    "    #print(conv_output =  [l for l in input_model.layers if l.name == layer_name][0].output)\n",
    "    \n",
    "    loss = KTF.sum(model.layers[-1].output)\n",
    "    print(\"~~~~~~\")\n",
    "\n",
    "    grads = normalize(KTF.gradients(loss, conv_output)[0])\n",
    "    gradient_function = KTF.function([model.layers[0].input], [conv_output, grads])\n",
    "    output, grads_val = gradient_function([image])\n",
    "    output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
    "    #多分GAP\n",
    "    weights = np.mean(grads_val, axis = (0, 1))\n",
    "    cam = np.ones(output.shape[0 : 2], dtype = np.float32)\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w*(255*output[:, :, i])\n",
    "\n",
    "    cam = cv2.resize(cam, (300, 300))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    heatmap = cam / np.max(cam)\n",
    "    \n",
    "    #Return to BGR [0..255] from the preprocessed image\n",
    "    image = image[0, :]\n",
    "    image -= np.min(image)\n",
    "    image = np.minimum(image, 255)\n",
    "\n",
    "    cam1 = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
    "    cam = np.float32(cam1) + np.float32(255*image)\n",
    "    cam = 255 * cam / np.max(cam)\n",
    "    cv2.imwrite(\"heat.jpg\",cam1)\n",
    "    return np.uint8(cam), heatmap\n",
    "\n",
    "\n",
    "\n",
    "def check_accuracy_rate(model):\n",
    "    path_and_label_and_weather = []\n",
    "    with open(\"./path_and_label_test.txt\",\"r\") as s:\n",
    "            image_urls=s.readlines()\n",
    "    \n",
    "    for i in image_urls:\n",
    "            url_and_label=i.strip(\" \").split(\" \")\n",
    "            image_url = url_and_label[0]\n",
    "            image_label=url_and_label[1]\n",
    "            img = image.load_img(image_url, target_size=(300,300))\n",
    "            img=image.img_to_array(img)\n",
    "            img.astype(\"float32\")\n",
    "            img/=255.0\n",
    "            predictions=model.predict(np.expand_dims(img,axis=0))\n",
    "            predicted_label=np.argmax(predictions)\n",
    "            if int(predicted_label) != int(image_label):\n",
    "                t=image_url.split(\"/\")\n",
    "                count=0\n",
    "                for s in t:\n",
    "                    if s==\"dataset_valid\":\n",
    "                        name=t[count+1]\n",
    "                    count+=1\n",
    "                path_and_label_and_weather.append([image_url,name,predicted_label,int(image_label)])\n",
    "                \n",
    "                with open(\"./doc/incorrect_test_images.txt\",\"a\") as f:\n",
    "                    f.write(image_url+\"\\n\")\n",
    "\n",
    "            elif int(predicted_label) == int(image_label) :\n",
    "                if (predictions[0,predicted_label] < np.float32(\"0.7\")):\n",
    "                    with open(\"./doc/corret_test_images.txt\",\"a\") as g:\n",
    "                        g.write(image_url+\"\\n\")\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    df = pd.DataFrame(path_and_label_and_weather,columns=[\"path\",\"weater\",\"predicted_label\",\"label\"])\n",
    "    #df=df.set_index(\"weater\")\n",
    "    df.to_csv(\"./doc/incorrect_test_images.csv\")\n",
    "\n",
    "# 1次元に引き延ばす(PCAで使用)                                                                                              \n",
    "def flatten_image(img):\n",
    "\n",
    "    s = img.shape[0] * img.shape[1] #* img.shape[2]\n",
    "    img_wide = img.reshape(1, s)\n",
    "    return img_wide[0]\n",
    "\n",
    "\n",
    "def handle_image_with_pca(activations, y_test, channel):\n",
    "    \"\"\"\n",
    "    分類分布をPCAにて取得\n",
    "    \"\"\"\n",
    "    for i in range(4):\n",
    "        images = activations[i,:,:,:]\n",
    "        if i == 0:\n",
    "            labels = y_test[:channel]\n",
    "        elif i == 1:\n",
    "            labels = y_test[channel:channel*2]\n",
    "        elif i == 2:\n",
    "            labels = y_test[channel*2:channel*3]\n",
    "        elif i == 3:\n",
    "            labels = y_test[channel*3:]\n",
    "        ls = []\n",
    "        for label in labels:\n",
    "\n",
    "            if list(label) == [1,0,0,0]:\n",
    "                ls.append(\"class_A\")\n",
    "            elif list(label) == [0,1,0,0]:\n",
    "                ls.append(\"class_B\")\n",
    "            elif list(label) == [0,0,1,0]:\n",
    "                ls.append(\"class_C\")\n",
    "            elif list(label) == [0,0,0,1]:\n",
    "                ls.append(\"class_D\")\n",
    "                \n",
    "        labels = ls\n",
    "        data = []\n",
    "        for a in range(channel):#for image in images:\n",
    "            img = flatten_image(images[:, :, a])\n",
    "            data.append(img)\n",
    "\n",
    "        data = np.array(data)\n",
    "        \"\"\"\n",
    "        is_train = np.random.uniform(0, 1, len(data)) <= 0.7\n",
    "        y = np.where(np.array(labels) == 'cloudy_seesaa', 1, 0)\n",
    "\n",
    "        train_x, train_y = data[is_train], y[is_train]\n",
    "        \"\"\"\n",
    "        \n",
    "        pca = PCA(n_components=2)\n",
    "        X = pca.fit_transform(data)\n",
    "\n",
    "        if i == 0:\n",
    "            df1 = pd.DataFrame({\"x\": X[:, 0], \"y\": X[:, 1],\n",
    "                               \"label\": np.full(channel,\"class_A\")})\n",
    "        elif i == 1:\n",
    "            df2 = pd.DataFrame({\"x\": X[:, 0], \"y\": X[:, 1],\n",
    "                               \"label\": np.full(channel,\"class_B\")})\n",
    "        elif i == 2:\n",
    "            df3 = pd.DataFrame({\"x\": X[:, 0], \"y\": X[:, 1],\n",
    "                               \"label\": np.full(channel,\"class_C\")})\n",
    "        elif i == 3:\n",
    "            df4 = pd.DataFrame({\"x\": X[:, 0], \"y\": X[:, 1],\n",
    "                               \"label\": np.full(channel,\"class_D\")})\n",
    "        \n",
    "    df = pd.concat([df1, df2,df3,df4])\n",
    "        \n",
    "    colors = ['red','blue','green','fuchsia']\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    print(df)\n",
    "    print(df['label'].unique())\n",
    "\n",
    "    for label, color in zip(df['label'].unique(), colors):\n",
    "        mask = df['label'] == label\n",
    "        plt.scatter(df[mask]['x'], df[mask]['y'], c=color, label=label)\n",
    "    sns.set()\n",
    "    plt.xlabel(\"pc1 (Principal Component1)\")  # 全データの分散が最大となる方向                                              \n",
    "    plt.ylabel(\"pc2 (Principal Component2)\")  # 第一主成分に垂直な方向の軸                                                  \n",
    "    plt.legend()\n",
    "    #plt.show()                                                                                                             \n",
    "    plt.savefig('pca_feature.png')\n",
    "    \"\"\"\n",
    "    # training a classifier                                                                                                 \n",
    "    pca = PCA(n_components=4)\n",
    "    train_x = pca.fit_transform(train_x)\n",
    "\n",
    "    svm = LinearSVC(C=1.0)\n",
    "    svm.fit(train_x, train_y)\n",
    "    joblib.dump(svm, 'model.pkl')\n",
    "\n",
    "    # evaluating the model                                                                                                  \n",
    "    test_x, test_y = data[is_train == False], y[is_train == False]\n",
    "    test_x = pca.transform(test_x)\n",
    "    print(pd.crosstab(test_y, svm.predict(test_x),\n",
    "                      rownames=['Actual'], colnames=['Predicted']))\n",
    "    \"\"\"\n",
    "\n",
    "def main():\n",
    "\n",
    "   # x_train, x_test, y_train, y_test, valid_data, valid_label = input_data(\"path_and_label_train.txt\",\n",
    "#                                                                              \"path_and_label_test.txt\")\n",
    "\n",
    "  #  old_session = KTF.get_session()\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session = tf.Session('')\n",
    "        \n",
    "        KTF.set_session(session)\n",
    "        \"\"\"\n",
    "        KTF.set_learning_phase(1)#(0 = test, 1 = train) これがGrad-CAMをダメにしていた！！！　Grad-CAM使うときはコメントアウトすること\n",
    "       # saver = tf.train.import_meta_graph(\"./tmp/model.ckpt.meta\")\n",
    "        #saver.restore(session, \"./tmp/model.ckpt\")\n",
    "\n",
    "        \n",
    "        model = Sequential()                                                                                                \n",
    "        model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape, kernel_initializer=\"he_normal\", bias_initializer=\"zeros\"))                                                                         \n",
    "        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))                                                                    \n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "        model.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "        model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Conv2D(256, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "        model.add(Conv2D(256, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "        model.add(Flatten())                                                                                                \n",
    "        model.add(Dense(512, activation='relu', init='he_uniform'))          # どのようにハイパーパラメータ512を決めるのか→https://stackoverflow.com/questions/36950394/how-to-decide-the-size-of-layers-in-keras-dense-method                                                \n",
    "        #model.add(Dropout(0.5))                                                                                             \n",
    "        model.add(Dense(num_classes, activation='softmax')) # num_classes = 2値分類                                         \n",
    "    \n",
    "                                                                                                                            \n",
    " \n",
    "       # load trained model                                                                                                \n",
    "        #json_string = open(os.path.join(f_model, model_filename)).read()\n",
    "        #model = model_from_json(json_string)\n",
    "        #model.load_weights(os.path.join(f_model, weights_filename))\n",
    "        \"\"\"\n",
    "        model = load_model('./model/test/Mymodel2.h5')\n",
    "        print(model.summary())\n",
    "     #   save_model_to_file(model,'4class_shisa_net.pdf')\n",
    "\n",
    "        \"\"\"\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,                                                           \n",
    "                      optimizer=\"SGD\",                                                                                      \n",
    "                      metrics=['accuracy'])                                                                                 \n",
    "                                                                                                                            \n",
    "        print(model.summary())                                                                                              \n",
    "                                                                                                                            \n",
    "        # callback function                                                                                                 \n",
    "        tb_cb = keras.callbacks.TensorBoard(log_dir=f_log, histogram_freq=1)                                                \n",
    "        cbks = [tb_cb]                                                                                                      \n",
    "                                                                                                                            \n",
    "        # train                                                                                                             \n",
    "        history = model.fit(x_train, y_train,                                                                               \n",
    "                            batch_size=batch_size,                                                                          \n",
    "                            epochs=epochs,                                                                                  \n",
    "                            verbose=1,       #進行状況の表示モード                                                          \n",
    "                            callbacks=cbks,  # [plot_losses, csv_logger],                                                   \n",
    "                            validation_data=(x_test, y_test))                                                               \n",
    "        score_train = model.evaluate(x_train,y_train, verbose=1, batch_size=batch_size)                                              \n",
    "        score_test = model.evaluate(x_test, y_test, verbose=1, batch_size=batch_size)                                                \n",
    "        print('Train loss: {0}'.format(score_train[0]))                                                                     \n",
    "        print('Train accuracy: {0}'.format(score_train[1]))                                                                 \n",
    "        print('Test loss: {0}'.format(score_test[0]))                                                                       \n",
    "        print('Test accuracy: {0}'.format(score_test[1]))                                                                   \n",
    "\n",
    "       # saver = tf.train.Saver()\n",
    "        #save_path=saver.save(session, \"./tmp/model.ckpt\")\n",
    "        model.save('./model/test/Mymodel2.h5')\n",
    "        \"\"\"\n",
    "\n",
    "        #テスト画像全ての正誤を取得\n",
    "        #check_accuracy_rate(model)\n",
    "    \n",
    "#        img = image.load_img(\"/home/seimei/brush.jpg\", target_size=(300, 300))\n",
    "\n",
    "    # add for TeonsorBoard                                                                                                                                  \n",
    "\n",
    "   # KTF.set_session(old_session)\n",
    "        url= \"/home/seimei/Graduation_Research/dataset_valid/hare/class_B5/image_0019.jpg\"\n",
    "    #\"/home/seimei/Graduation_Research/dataset/hare/hare_D2/image_0037.jpg\"\n",
    "    #'/home/seimei/Graduation_Research/dataset/kumori/kumori_D2/image_0001.jpg'  なるほどね\n",
    "    #\"/home/seimei/Graduation_Research/dataset_valid/hare/hare_D4/image_0189.jpg\"\n",
    "    #\"/home/seimei/Graduation_Research/dataset_valid/hare/class_B5/image_0019.jpg\"\n",
    "    #\"/home/seimei/Graduation_Research/dataset_valid/hare/hare_D4/image_0024.jpg\"\n",
    "        img = image.img_to_array(img_to_matrix(url))\n",
    "        img.astype('float32')\n",
    "        img /= 255.0\n",
    "        img = np.expand_dims(img,axis=0)\n",
    "           # preprocessed_input = load_image(\"/home/seimei/image_0058_brush.jpg\")\n",
    "        predictions=model.predict(img)#np.expand_dims(img, axis=0))\n",
    "\n",
    "        \n",
    "        predicted_class= np.argmax(predictions)\n",
    "        print(predicted_class)\n",
    "        print(predictions)\n",
    "        cam, heatmap = grad_cam(model, img, predicted_class, \"conv2d_6\")\n",
    "        cv2.imwrite(\"gradcam.jpg\", cam)\n",
    "        cv2.imwrite(\"test_heatmap.jpg\", heatmap)\n",
    "        register_gradient()\n",
    "        guided_model = modify_backprop(model, 'GuidedBackProp')\n",
    "        saliency_fn = compile_saliency_function(guided_model)\n",
    "        saliency = saliency_fn([img,0])#[np.expand_dims(img, axis=0), 0])\n",
    "        print(saliency)\n",
    "        gradcam = saliency[0] * heatmap[..., np.newaxis]\n",
    "        #  print(type(gradcam))\n",
    "        cv2.imwrite(\"guided_gradcam.jpg\", deprocess_image(gradcam))\n",
    "        \"\"\"\n",
    "        \n",
    "   #     ========================================================== 重みの可視化       \n",
    "        img = image.load_img(\"/home/seimei/Graduation_Research/dataset_valid/hare/class_B5/image_0019.jpg\", target_size=(300, 300))  # 画像を読み込む。\n",
    "        x = image.img_to_array(img)  # PIL オブジェクトを numpy 配列にする。\n",
    "        x = np.expand_dims(x, axis=0)  # ミニバッチにするため、次元を追加する。\n",
    "        x = preprocess_input(x)  # ResNet 用の前処理を行う。\n",
    "\n",
    "        # モデルの3層目の出力を返す関数を作成する。\n",
    "        get_feature_map = KTF.function([model.input, KTF.learning_phase()], [model.layers[3].output])\n",
    "        print(model.layers[3].output)\n",
    "        # 順伝搬して特徴マップを取得する。\n",
    "        features = get_feature_map([x, False])[0]\n",
    "        print('features.shape', features.shape)  # features.shape (1, 112, 112, 64)\n",
    "\n",
    "        # 重みを取得する。\n",
    "        [weights, bias] = model.layers[3].get_weights()\n",
    "        print(model.layers[3])\n",
    "        print('layer.name', model.layers[3].name)  # layer.name bn_conv1\n",
    "        print('weights.shape', weights.shape)  # weights.shape (7, 7, 3, 64)\n",
    "        print('bias.shape', bias.shape)  # bias.shape (64,)\n",
    "\n",
    "\n",
    "        # 特徴マップをカーネルごとに分割し、画像化する。\n",
    "        feature_imgs = []\n",
    "        for f in np.split(features, 64, axis=3):\n",
    "            f = np.squeeze(f, axis=0)  # (1, FeatureH, FeatureW, FeatureC) -> (FeatureH, FeatureW, FeatureC)\n",
    "            f = image.array_to_img(f)  # 特徴マップを画像化する。\n",
    "            f = np.array(f)  # PIL オブジェクトを numpy 配列にする。\n",
    "            feature_imgs.append(f)\n",
    "        \n",
    "        weights = weights.transpose(3,2,0,1) #配列を転置\n",
    "        nb_filter, nb_channel, nb_row, nb_col = weights.shape\n",
    "        print(\"nb_filter:{}\".format(nb_filter), \"nb_channel:{}\".format(nb_channel),\"nb_row:{}\".format(nb_row),\"nb_col:{}\".format(nb_col))\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        plt.figure()\n",
    "        for i in range(nb_filter):\n",
    "            im=weights[i,0]\n",
    "            #重みを0-255のスケールに変換\n",
    "           # scaler=MinMaxScaler(feature_range=(0,255))\n",
    "           # im = scaler.fit_transform(im)\n",
    "            #プロットを1つの図にまとめる \n",
    "\n",
    "            plt.subplot(8,8,i+1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(im, cmap='gray')\n",
    "        plt.savefig(\"weight.png\")\n",
    "        plt.show()\n",
    "\n",
    "        cols = 6\n",
    "        rows = int(len(feature_imgs) / cols)\n",
    "        fig, axes = plt.subplots(rows, cols * 2, figsize=(100, 100))\n",
    "\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                i = r * cols + c\n",
    "                w_axis, f_axis = axes[r, c * 2], axes[r, c * 2 + 1]\n",
    "\n",
    "                w_axis.imshow(weights[i,0],cmap='gray')\n",
    "                w_axis.set_title('Kernel')\n",
    "                w_axis.axis('off')\n",
    "\n",
    "                f_axis.imshow(feature_imgs[i], cmap='gray')\n",
    "                f_axis.set_title('Feature')\n",
    "                f_axis.axis('off')\n",
    "        plt.savefig(\"weight2.png\")\n",
    "        plt.show()\n",
    "\n",
    "     ##   =============================================================中間層活性化の可視化\n",
    "\n",
    "        layer_name =\"conv2d_2\"# \"max_pooling2d_1\"                                                                                    \n",
    "        intermediate_layer_model = keras.models.Model(inputs=model.input,\n",
    "                                         outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "\n",
    "        layers = model.layers[1:2]     \n",
    "        \n",
    "        urls = [\"/home/seimei/Graduation_Research/dataset_valid/kumori/class_A5/image_0005.jpg\",\n",
    "                \"/home/seimei/Graduation_Research/dataset_valid/hare/class_B5/image_0019.jpg\", \n",
    "                \"/home/seimei/Graduation_Research/dataset_valid/kumori/kumori_C5/image_0001.jpg\",\n",
    "        \"/home/seimei/Graduation_Research/dataset/hare/hare_D2/image_0037.jpg\"]\n",
    "        \n",
    "        width=300\n",
    "        high=300\n",
    "        channel=32\n",
    "        activations = np.zeros((0,width,high,channel))\n",
    "        for url in urls:\n",
    "            img = image.load_img(url, target_size=(300, 300))\n",
    "            img = image.img_to_array(img)\n",
    "            img /= 255\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            # 指定したlayer_nameと一致するレイヤーの出力を取得する                                                                                              \n",
    "            _activations = intermediate_layer_model.predict(img)\n",
    "            _activations = [activation for layer, activation in zip(layers, _activations) if isinstance(layer, Conv2D)]\n",
    "            print(np.shape(_activations))\n",
    "            activations = np.r_[activations, np.reshape(_activations,(-1,width,high,channel))]\n",
    "        print(np.shape(activations))\n",
    "\n",
    "        # 出力層ごとに特徴画像を並べてヒートマップ画像として出力                                                                                            \n",
    "        for i, activation in enumerate(activations):\n",
    "            num_of_image = activation.shape[2]\n",
    "            cols = math.ceil(math.sqrt(num_of_image))\n",
    "            rows = math.floor(num_of_image / cols)\n",
    "            screen = []\n",
    "            for y in range(0, rows):\n",
    "                row = []\n",
    "                for x in range(0, cols):\n",
    "                    j = y * cols + x\n",
    "                    if j < num_of_image:\n",
    "                        row.append(activation[:, :, j])\n",
    "                    else:\n",
    "                        row.append(np.zeros())\n",
    "                screen.append(np.concatenate(row, axis=1))\n",
    "            screen = np.concatenate(screen, axis=0)\n",
    "            plt.figure(figsize=(50, 50))\n",
    "            sns.heatmap(screen, xticklabels=False, yticklabels=False)\n",
    "            name = \"maxpooling2d\"\n",
    "            plt.savefig(\"%s.png\" % name)\n",
    "            plt.plot()\n",
    "            plt.close()\n",
    "\n",
    "        #PCA\n",
    "        test_labels = keras.utils.to_categorical(np.r_[np.zeros(channel), np.ones(channel),np.full(channel,2),np.full(channel,3)])\n",
    "        handle_image_with_pca(activations, test_labels, channel)\n",
    "        \"\"\"\n",
    "        session.close()\n",
    "    sess.close()\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 1152391751511621215, name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 14299281323509105975\n",
       " physical_device_desc: \"device: XLA_CPU device\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNにおける最終層の重み可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "%tensorboard --logdir==./logs/object_classificationAB/events.out.tfevents.1549894103.1080ti "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
